# -*- coding: utf-8 -*-
"""CourseTA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nqj7rJGTQEeh-4veqHMFEPWS2Npxyccl

#Course work (Text analysis)

###Sentiment analysis of posts from the VK-group "Netfleet"

–ú–æ—è —Ä–∞–±–æ—Ç–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –∞–Ω–∞–ª–∏–∑–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å—Ç–æ–≤ –≤ –í–ö-–≥—Ä—É–ø–ø–µ –ø–æ –≤–æ–¥–Ω–æ–º—É —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç—É. –ü–∞—Ä—Å–∏–ª–∞ –ø–æ—Å—Ç—ã —è –ª–æ–∫–∞–ª—å–Ω–æ –≤ jupyter –∏ –≤—ã–≥—Ä—É–∑–∏–ª–∞ –≤ csv —Ñ–∞–π–ª.
"""

!pip install emoji demoji googletrans==3.1.0a0 -q
!pip install pymorphy3 -q
!pip install transformers torch accelerate -q
!pip install wordcloud -q
!pip install imbalanced-learn -q

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import re
import emoji
import demoji
from googletrans import Translator

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

import pymorphy3
from tqdm.auto import tqdm
from wordcloud import WordCloud
from collections import Counter

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import RandomOverSampler

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

from transformers import pipeline

demoji.download_codes()
translator = Translator()
tqdm.pandas()

"""##EDA

"""

df = pd.read_csv('vk_posts_group2.csv', header=None, names=['text'])
df.head()

df.shape

#—É–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ + —Å—Ç—Ä–æ–∫–∏ –∏–∑ –ø—Ä–æ–±–µ–ª–æ–≤ + —Å—É–ø–µ—Ä –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
df = df.dropna()

df = df[df['text'].str.strip() != '']

df['text'] = df['text'].astype(str)
df = df[df['text'].str.len() > 1]
df['text'] = df['text'].astype(str)

print(f'–¢–∞–±–ª–∏—Ü–∞ –±–µ–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Å–æ–¥–µ—Ä–∂–∏—Ç {df.shape[0]} —Å—Ç—Ä–æ–∫\n')

df = df.drop_duplicates(subset=['text'])

print(f'–¢–∞–±–ª–∏—Ü–∞ –±–µ–∑ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –ø–æ—Å—Ç–æ–≤ —Å–æ–¥–µ—Ä–∂–∏—Ç {df.shape[0]} —Å—Ç—Ä–æ–∫\n')

"""**–ü–æ—Å–º–æ—Ç—Ä–∏–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω –ø–æ—Å—Ç–æ–≤**"""

df['length'] = df['text'].str.len()
df['length'].describe()

plt.figure(figsize=(10,6))
plt.hist(df['length'], bins=100, color='lightblue')
plt.xlim(0, 3000)
plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –ø–æ—Å—Ç–æ–≤')
plt.xlabel('–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞')
plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤')
plt.show()

import seaborn as sns

plt.figure(figsize=(15,5))
sns.kdeplot(df['length'], fill=True)
plt.xlim(0, 3000)
plt.title('–ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–ª–∏–Ω—ã –ø–æ—Å—Ç–æ–≤')
plt.show()

#—Å–∞–º—ã–µ –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–æ—Å—Ç—ã
df.sort_values('length').head(10)

#—Ç–æ–ø-10 –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤
df[['text', 'length']].sort_values('length').head(10)
df[['text', 'length']].sort_values('length', ascending=False).head(10)

"""###–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–º–∞–π–ª–∏–∫–æ–≤

–°–æ–±–∏—Ä–∞–µ–º —Å–º–∞–π–ª–∏–∫–∏ –¥–ª—è –≤—Å–µ—Ö –ø–æ—Å—Ç–æ–≤
"""

def extract_emojis(s):
    return ''.join(c for c in s if c in emoji.EMOJI_DATA)

set_text = df['text'].astype(str).tolist()

#—Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ –≥–¥–µ –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ ‚Äî —Ç–æ–ª—å–∫–æ —Å–º–∞–π–ª–∏–∫–∏ –∏–∑ –ø–æ—Å—Ç–∞
emoji_chat = [extract_emojis(s) for s in set_text]

df['emojis'] = emoji_chat

"""–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–º–∞–π–ª–∏–∫–æ–≤"""

def emojis_to_desc_en(emo_str: str) -> str:
    #emo_str —Å—Ç—Ä–æ–∫–∞ —Ç–∏–ø–∞ 'üòä'
    if not emo_str:
        return ''
    dict_emoj = demoji.findall(emo_str)  #–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Ç–∏–ø–∞ {'üòä': '—Å–º–∞–π–ª–∏–Ω–≥ —Ñ–µ–π—Å'}

    res = []
    for key in dict_emoj.keys():
        res.append(dict_emoj[key])

    #—Å–∫–ª–µ–∏–≤–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏—è –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É
    return ' '.join(res)

df['emoji_desc_en'] = df['emojis'].apply(emojis_to_desc_en)

df[df['emojis'] != ''][['text', 'emojis', 'emoji_desc_en']].head(10)

"""–ü–µ—Ä–µ–≤–æ–¥ –æ–ø–∏—Å–∞–Ω–∏—è —Å–º–∞–π–ª–∏–∫–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–∏–π"""

translator = Translator()

def translate_to_ru(text: str) -> str:
    text = text.strip()
    if not text:
        return ''
    try:
        result = translator.translate(text, dest='ru')
        return result.text
    except:
        #–µ—Å–ª–∏ –≥—É–≥–ª —à–ª–µ—Ç –æ—à–∏–±–∫–∏ –æ—Å—Ç–∞–≤–ª—è–µ–º –∞–Ω–≥–ª –≤–∞—Ä–∏–∫
        return text

df['emoji_desc_ru'] = df['emoji_desc_en'].progress_apply(translate_to_ru)

"""–°–æ–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–æ–≤ –∏ –ø–µ—Ä–µ–≤–æ–¥ —Å–º–∞–π–ª–∏–∫–æ–≤"""

df['text_with_emoji'] = df['text'] + ' ' + df['emoji_desc_ru'].fillna('')

"""##–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤"""

morph = pymorphy3.MorphAnalyzer()
stop_ru = set(stopwords.words("russian"))
stop_en = set(stopwords.words("english"))

def preprocess_text(text):
    if not isinstance(text, str):
        text = str(text)

    text = re.sub(r'http\S+', ' ', text)
    text = re.sub(r'#\S+', ' ', text)
    text = re.sub(r'\b(?:www\.)?\w+\.(?:ru|com|org|net|info|club|online|site|io|gov|edu)\b', ' ', text)
    text = re.sub(r"[^–∞-—è–ê-–Ø—ë–Åa-zA-Z\s]", " ", text)
    text = text.lower()

    tokens = word_tokenize(text, language="russian")
    tokens = [w for w in tokens if w.isalpha()]

    #—É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤, –∫—Ä–æ–º–µ "–Ω–µ"
    clean_tokens = []
    for w in tokens:
        if w == "–Ω–µ":
            clean_tokens.append(w)
            continue
        if w in stop_ru or w in stop_en:
            continue
        clean_tokens.append(w)

    lemmas = []
    for w in clean_tokens:
        if re.fullmatch(r"[–∞-—è—ë]+", w):
            lemmas.append(morph.parse(w)[0].normal_form)
        elif re.fullmatch(r"[a-z]+", w):
            lemmas.append(w)
        else:
            continue

    lemmas = [w for w in lemmas if (len(w) > 1 or w == "–Ω–µ")]

    if lemmas:
        return " ".join(lemmas)

    #—Ñ–æ–ª–±—ç–∫
    text2 = re.sub(r"[^–∞-—è–ê-–Ø—ë–Åa-zA-Z\s]", " ", text)
    out = " ".join([w for w in text2.split() if w.isalpha()])

    return out if out else ""

tqdm.pandas()
df['processed_text'] = df['text_with_emoji'].progress_apply(preprocess_text)

df[df['emojis'] != ''][['text', 'emojis', 'emoji_desc_ru', 'processed_text']].head()

"""##–û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –∫–æ—Ä–ø—É—Å–∞

–°–∞–º—ã–µ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞
"""

all_tokens = []

for text in df['processed_text']:
    tokens = re.findall(r'[–∞-—è—ëa-z]+', str(text).lower())
    all_tokens.extend(tokens)

word_counts = Counter(all_tokens).most_common(50)
freq_df = pd.DataFrame(word_counts, columns=['—Å–ª–æ–≤–æ', '—á–∞—Å—Ç–æ—Ç–∞'])
freq_df = freq_df[freq_df['—Å–ª–æ–≤–æ'] != '–Ω–µ']  #—á—Ç–æ–±—ã "–Ω–µ" –Ω–µ –∑–∞–±–∏–≤–∞–ª–æ –≤—Å–µ

plt.figure(figsize=(10,5))
sns.barplot(data=freq_df.head(10), x='—Å–ª–æ–≤–æ', y='—á–∞—Å—Ç–æ—Ç–∞', color = 'lightblue')
plt.title('–¢–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö —Å–ª–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ')
plt.xlabel('–°–ª–æ–≤–æ')
plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
plt.xticks(rotation=45)
plt.show()

"""–û–±–ª–∞–∫–æ —Å–ª–æ–≤"""

text_all = " ".join(df['processed_text'])

text_all = " ".join([w for w in all_tokens if w != '–Ω–µ']) #–æ–ø—è—Ç—å –∂–µ —Ç–∫ –±—É–¥–µ—Ç –ª–∏—à–Ω–∏–º

wc = WordCloud(width=1200, height=600, background_color='white', collocations=False).generate(text_all)

plt.figure(figsize=(15,7))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.show()

"""–°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –±–∏–≥—Ä–∞–º–º—ã"""

#—Å—á–∏—Ç–∞–µ–º –±–∏–≥—Ä–∞–º–º—ã
vec = CountVectorizer(ngram_range=(2, 2), min_df=5)
X_bi = vec.fit_transform(df['processed_text'])

#—Å—É–º–º–∏—Ä—É–µ–º —á–∞—Å—Ç–æ—Ç—ã –±–∏–≥—Ä–∞–º–º –∏ —Å–æ–±–∏—Ä–∞–µ–º —Ç–∞–±–ª–∏—Ü—É
bi_df = pd.DataFrame({
    '–±–∏–≥—Ä–∞–º–º–∞': vec.get_feature_names_out(),
    '—á–∞—Å—Ç–æ—Ç–∞': X_bi.toarray().sum(axis=0)
})

bi_df = bi_df.sort_values('—á–∞—Å—Ç–æ—Ç–∞', ascending=False).reset_index(drop=True)
bi_df = bi_df[~bi_df['–±–∏–≥—Ä–∞–º–º–∞'].str.contains('–≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π|–∑–Ω–∞–∫|–∫—Ä–∞—Å–Ω—ã–π')]

plt.figure(figsize=(12, 6))
sns.barplot(data=bi_df.head(10), x='–±–∏–≥—Ä–∞–º–º–∞', y='—á–∞—Å—Ç–æ—Ç–∞', color='lightblue')
plt.title('–¢–æ–ø-10 –±–∏–≥—Ä–∞–º–º', fontsize=14, pad=20)
plt.xlabel('–ë–∏–≥—Ä–∞–º–º–∞')
plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""##–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Å BERT"""

tqdm.pandas()

sentiment_model = pipeline("sentiment-analysis", model="seara/rubert-tiny2-russian-sentiment", return_all_scores=True)

THRESH = 0.45  #–ø–æ—Ä–æ–≥ –¥–ª—è positive/negative

def get_sentiment_thresh(text: str, thresh: float = THRESH) -> str:
    if not isinstance(text, str):
        text = str(text)
    text = text.strip()
    if not text:
        return "neutral"
    try:
        scores = sentiment_model(text)[0]   #—Å–ø–∏—Å–æ–∫ –∏–∑ dict–æ–≤ [{'label':..., 'score':...}, ...]
        #–ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —É–¥–æ–±–Ω—ã–π –≤–∏–¥: label -> score
        score_dict = {d['label'].lower(): d['score'] for d in scores}
        pos = score_dict.get('positive', 0.0)
        neg = score_dict.get('negative', 0.0)
        neu = score_dict.get('neutral', 0.0)

        #–µ—Å–ª–∏ positive –∏–ª–∏ negative >= thresh ‚Äî —Å—á–∏—Ç–∞–µ–º —ç—Ç–æ —ç–º–æ—Ü–∏–µ–π
        #–µ—Å–ª–∏ –æ–±–∞ –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞ ‚Äî –±–µ—Ä—ë–º –∏–∑ –Ω–∏—Ö —Ç–æ—Ç, —á—Ç–æ –±–æ–ª—å—à–µ
        if pos >= thresh and neg < thresh:
            return 'positive'
        elif neg >= thresh and pos < thresh:
            return 'negative'
        elif pos >= thresh and neg >= thresh:
            #–æ–±–∞ –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞ ‚Äî –±–µ—Ä—ë–º –±–æ–ª–µ–µ —Å–∏–ª—å–Ω—ã–π
            return 'positive' if pos >= neg else 'negative'
        else:
            #–Ω–∏ –ø–æ–∑–∏—Ç–∏–≤, –Ω–∏ –Ω–µ–≥–∞—Ç–∏–≤ –Ω–µ –¥–æ—Ç—è–Ω—É–ª–∏ ‚Äî —Å—á–∏—Ç–∞–µ–º –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–º
            return 'neutral'

    except Exception:
        return "neutral"

df['sentiment'] = df['text_with_emoji'].progress_apply(get_sentiment_thresh)
df[['text', 'sentiment']].head()

"""**–ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø–æ—Å—Ç—ã**"""

df['sentiment'].value_counts(), df['sentiment'].value_counts(normalize=True).round(3)

sent_counts = df['sentiment'].value_counts()

plt.figure(figsize=(6,4))
sent_counts.plot(kind='bar', color = 'lightblue')
plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π –ø–æ—Å—Ç–æ–≤')
plt.xlabel('–ö–ª–∞—Å—Å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏')
plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤')
plt.xticks(rotation=0)
plt.show()

"""##–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

###LogisticRegression —Å –æ–≤–µ—Ä—Å—ç–º–ø–ª–∏–Ω–≥–æ–º
"""

VOCABULARY_SIZE = 10000

X = df['processed_text']
y = df['sentiment']

#–æ–±—â–∏–π train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º train:")
print(y_train.value_counts())

#–æ–≤–µ—Ä—Å—ç–º–ø–ª–∏–Ω–≥
train_df = pd.DataFrame({'text': X_train, 'sentiment': y_train})

neu = train_df[train_df['sentiment'] == 'neutral']
pos = train_df[train_df['sentiment'] == 'positive']
neg = train_df[train_df['sentiment'] == 'negative']

#—Ü–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä - –Ω–µ–π—Ç—Ä–∞–ª–∫–∞
target_per_class = len(neu)

pos_up = pos.sample(target_per_class, replace=True, random_state=42)
neg_up = neg.sample(target_per_class, replace=True, random_state=42)

train_balanced = pd.concat([neu, pos_up, neg_up]) \
    .sample(frac=1, random_state=42) \
    .reset_index(drop=True)

print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ upsampled train:")
print(train_balanced['sentiment'].value_counts())

#–æ–±—â–∏–π tf-idf
tfidf_vectorizer = TfidfVectorizer(max_features=VOCABULARY_SIZE, ngram_range=(1, 2))

#–ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ train/test
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

#–ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –¥–ª—è –≤–∞—Ä–∏–∞–Ω—Ç–∞ —Å oversampling
X_train_tfidf_up = tfidf_vectorizer.transform(train_balanced['text'])

print("\n–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å TF-IDF –º–∞—Ç—Ä–∏—Ü—ã train (upsampled):", X_train_tfidf_up.shape)

#1 –ª–æ–≥—Ä–µ–≥ –±–µ–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞
logreg_plain = LogisticRegression(max_iter=1000, random_state=42)
logreg_plain.fit(X_train_tfidf, y_train)

y_pred_plain = logreg_plain.predict(X_test_tfidf)

print("–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –±–µ–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞")
print("Accuracy (plain logreg):", accuracy_score(y_test, y_pred_plain))
print(classification_report(y_test, y_pred_plain, digits=4))


#2 –ª–æ–≥—Ä–µ–≥ —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –≤–µ—Å–æ–≤
logreg_bal = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)
logreg_bal.fit(X_train_tfidf, y_train)

y_pred_bal = logreg_bal.predict(X_test_tfidf)

print("\n–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –≤–µ—Å–æ–≤")
print("Accuracy (logreg + weights):", accuracy_score(y_test, y_pred_bal))
print(classification_report(y_test, y_pred_bal, digits=4))


#3 –ª–æ–≥—Ä–µ–≥ —Å ovesampling
logreg_up = LogisticRegression(max_iter=1000, random_state=42)
logreg_up.fit(X_train_tfidf_up, train_balanced['sentiment'])

y_pred_up = logreg_up.predict(X_test_tfidf)

print("\n–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å oversampling")
print("Accuracy (logreg + oversampling):", accuracy_score(y_test, y_pred_up))
print(classification_report(y_test, y_pred_up, digits=4))

"""–æ—á–µ–≤–∏–¥–Ω–æ, —á—Ç–æ –ª—É—á—à–µ –≤—Å–µ—Ö —Å–µ–±—è –ø–æ–∫–∞–∑–∞–ª–∞ –º–æ–¥–µ–ª—å —Å –æ–≤–µ—Ä—Å—ç–º–ø–ª–∏–Ω–≥–æ–º, —Ç–∫ –æ–Ω–∞ –¥–∞–µ—Ç –±–æ–ª–µ–µ —Ä–æ–≤–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ –≤—Å–µ–º —Ç—Ä–µ–º –∫–ª–∞—Å—Å–∞–º. —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –ø—Ä–∏–µ–º–ª–µ–º–∞—è –æ–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (0.79), –Ω–æ –º–æ–¥–µ–ª—å —É–∂–µ –Ω–µ –∑–∞–±—ã–≤–∞–µ—Ç –ø—Ä–æ —Ä–µ–¥–∫–∏–π –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –∫–ª–∞—Å—Å.

**–ü–æ—Å–º–æ—Ç—Ä–∏–º –∫–∞–∫–∏–µ –æ—à–∏–±–∫–∏ –¥–æ–ø—É—Å—Ç–∏–ª–∞ –º–æ–¥–µ–ª—å–∫–∞**
"""

cm_logreg = confusion_matrix(y_test, y_pred_up, labels=labels)

cm_logreg_df = pd.DataFrame(
    cm_logreg,
    index=[f"true_{lbl}" for lbl in labels],
    columns=[f"pred_{lbl}" for lbl in labels]
)

print("\n–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (logreg + oversampling):")
print(cm_logreg_df)

logreg_errors = (y_test.values != y_pred_up).sum()
print("\n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ (logreg + oversampling):", logreg_errors)

"""###MLP+oversampling"""

le = LabelEncoder()
le.fit(y)

#–∫–æ–¥–∏—Ä—É–µ–º train/test –º–µ—Ç–∫–∏
y_train_enc = le.transform(y_train)
y_test_enc = le.transform(y_test)

print(X_train_tfidf.shape, X_test_tfidf.shape)
print(pd.Series(y_train_enc).value_counts())

ros = RandomOverSampler(random_state=42)
X_train_over, y_train_over = ros.fit_resample(X_train_tfidf, y_train_enc)

print("–§–æ—Ä–º–∞ –ø–æ—Å–ª–µ oversampling:", X_train_over.shape)
print(pd.Series(y_train_over).value_counts())

X_train_over_dense = X_train_over.astype('float32').toarray()
X_test_dense = X_test_tfidf.astype('float32').toarray()

num_features = X_train_over_dense.shape[1]
num_classes = len(le.classes_)

print(num_features, num_classes)
tf.random.set_seed(42)

nn_model = Sequential([
    Dense(256, activation='relu', input_shape=(num_features,)),
    Dropout(0.5),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

nn_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

nn_model.summary()

history = nn_model.fit(
    X_train_over_dense,
    y_train_over,
    epochs=10,
    batch_size=64,
    validation_split=0.2,
    verbose=2
)

y_pred_proba = nn_model.predict(X_test_dense)
y_pred_nn = y_pred_proba.argmax(axis=1)

print("Accuracy (NN + oversampling):", accuracy_score(y_test_enc, y_pred_nn), 4)

print(classification_report(
    y_test_enc,
    y_pred_nn,
    target_names=le.classes_,
    digits=4
))

"""–ü–æ—Å–º–æ—Ç—Ä–∏–º –∫–∞–∫–∏–µ –æ—à–∏–±–∫–∏ –¥–æ–ø—É—Å—Ç–∏–ª–∞ –º–æ–¥–µ–ª—å"""

cm_nn = confusion_matrix(y_test_enc, y_pred_nn)

cm_nn_df = pd.DataFrame(
    cm_nn,
    index=[f"true_{lbl}" for lbl in le.classes_],
    columns=[f"pred_{lbl}" for lbl in le.classes_]
)

print("\n–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (–Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å):")
print(cm_nn_df)

#–∫–æ–ª-–≤–æ –æ—à–∏–±–æ–∫
nn_errors = (y_test_enc != y_pred_nn).sum()
print("\n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ (–Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å):", nn_errors)